{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPeulvp-meob"
      },
      "source": [
        "**Classification Emperical Study**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8vwHE0G_iOG"
      },
      "source": [
        "**Dataset**\n",
        "\n",
        "This dataset emerges from a comprehensive collection initiative targeting various healthcare institutions, encompassing hospitals, community clinics, and maternal healthcare centers within rural regions of Bangladesh. The distinctive aspect of this data accumulation pertains to its method of collection through an IoT (Internet of Things) based risk monitoring system, ensuring real-time and accurate data.\n",
        "\n",
        "Dataset Characteristics:\n",
        "\n",
        "Type: Multivariate\n",
        "Number of Instances: 1013\n",
        "Number of Features: 6\n",
        "Subject Area:\n",
        "\n",
        "Life Science / Healthcare\n",
        "Associated Tasks:\n",
        "\n",
        "Classification\n",
        "Feature Type:\n",
        "\n",
        "Real, Integer\n",
        "Dataset Information:\n",
        "The dataset is robust, containing 1013 instances that integrate to form a comprehensive picture regarding maternal health risks. Each instance is characterized by six features, all of which are integral to understanding and mitigating the risks associated with maternal mortality - a critical issue underscored in the United Nations' Sustainable Development Goals (SDGs).\n",
        "\n",
        "Features Description:\n",
        "\n",
        "Age: Quantitative representation of the mother's age.\n",
        "SystolicBP (Systolic Blood Pressure): Continuous measurement indicating the maximum arterial pressure during contraction of the left ventricle of the heart.\n",
        "DiastolicBP (Diastolic Blood Pressure): Continuous measurement indicating the arterial pressure during the relaxation and dilatation of the heart's ventricles.\n",
        "BS (Blood Sugar): Quantitative measure of the concentration of glucose present in the mother's blood.\n",
        "BodyTemp (Body Temperature): Continuous representation of the mother's core body temperature.\n",
        "HeartRate: Integer value indicating the frequency of the mother's heartbeat, represented as beats per minute.\n",
        "RiskLevel: Categorical assessment of the maternal health risk, pivotal for classificatory tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZTWy1qN2BzY"
      },
      "source": [
        "**Import important libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GmP1buROhaOx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, cross_validate\n",
        "from sklearn.metrics import make_scorer, precision_score, recall_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reading the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BrhpM-HwhaOy"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/MehdiRih/Classification-Empirical-Study-Naive-Bayes-vs-Logistic-Regression/main/dataset/Maternal%20Health%20Risk%20Data%20Set.csv\"\n",
        "dataset = pd.read_csv(url)\n",
        "\n",
        "#dataset for Naive Bayes\n",
        "dataset_nb = dataset.copy()\n",
        "\n",
        "#dataset for Logistic Regression\n",
        "dataset_lr = dataset.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Encoding For Logistic Regression**\n",
        "\n",
        "For the Logistic Regression version, the RiskLevel column, which is categorical, is manually encoded into numerical values based on a predefined mapping: 'low risk' is mapped to 0, 'mid risk' to 1, and 'high risk' to 2. We also standardize the continuous features. This is done using the StandardScaler, ensuring that these features have a mean of 0 and a standard deviation of 1. Standardizing is crucial for algorithms like Logistic Regression, as it ensures that all features contribute equally to the model's decision-making process and optimizes the algorithm's convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "risk_mapping = {'low risk': 0, 'mid risk': 1, 'high risk': 2}\n",
        "\n",
        "# Standardize continuous features for Logistic Regression\n",
        "features = [\"Age\", \"SystolicBP\", \"DiastolicBP\", \"BS\", \"BodyTemp\", \"HeartRate\"]\n",
        "scaler = StandardScaler()\n",
        "dataset_lr[features] = scaler.fit_transform(dataset_lr[features])\n",
        "\n",
        "# Manually encode 'RiskLevel' for Logistic Regression\n",
        "dataset_lr['RiskLevel_encoded'] = dataset_lr['RiskLevel'].replace(risk_mapping)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Encoding for Naïve Bayes**\n",
        "\n",
        "For the Naive Bayes version, the RiskLevel column, which is categorical, is manually encoded into numerical values based on a predefined mapping: 'low risk' is mapped to 0, 'mid risk' to 1, and 'high risk' to 2. And since we are going to be using Gaussian Naïve Bayes and the features are all continuous we just need to make sure there is a level of normality in the distribution of the features. \n",
        "We apply the Shapiro-Wilk test to do an evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapiro-Wilk Test for Age:\n",
            "Statistic: 0.9160889983177185, P-value: 2.9400833315812626e-23\n",
            "\n",
            "Shapiro-Wilk Test for SystolicBP:\n",
            "Statistic: 0.9043952226638794, P-value: 1.1139871662946835e-24\n",
            "\n",
            "Shapiro-Wilk Test for DiastolicBP:\n",
            "Statistic: 0.946744978427887, P-value: 1.1778182349136168e-18\n",
            "\n",
            "Shapiro-Wilk Test for BS:\n",
            "Statistic: 0.673654317855835, P-value: 2.914168312379176e-40\n",
            "\n",
            "Shapiro-Wilk Test for BodyTemp:\n",
            "Statistic: 0.5276755094528198, P-value: 1.401298464324817e-45\n",
            "\n",
            "Shapiro-Wilk Test for HeartRate:\n",
            "Statistic: 0.9054552912712097, P-value: 1.4794961045751373e-24\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset_nb['RiskLevel_encoded'] = dataset_nb['RiskLevel'].replace(risk_mapping)\n",
        "\n",
        "# Shapiro-Wilk Test for normality\n",
        "for feature in features:\n",
        "    shapiro_test = stats.shapiro(dataset_nb[feature])\n",
        "    print(f\"Shapiro-Wilk Test for {feature}:\\nStatistic: {shapiro_test[0]}, P-value: {shapiro_test[1]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "P-value less than 0.05 typically indicates that the distribution is not normal.\n",
        "We use the Yeo-Johnson transformation to normalise the data and test again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Age', 'SystolicBP', 'DiastolicBP', 'BS', 'BodyTemp', 'HeartRate']\n",
            "Shapiro-Wilk Test for Age:\n",
            "Statistic: 0.9736714363098145, P-value: 1.3420394353105825e-12\n",
            "\n",
            "Shapiro-Wilk Test for SystolicBP:\n",
            "Statistic: 0.9067901968955994, P-value: 2.1223381659864477e-24\n",
            "\n",
            "Shapiro-Wilk Test for DiastolicBP:\n",
            "Statistic: 0.9467387795448303, P-value: 1.1747998528584108e-18\n",
            "\n",
            "Shapiro-Wilk Test for BS:\n",
            "Statistic: 0.9210563898086548, P-value: 1.3116011617875257e-22\n",
            "\n",
            "Shapiro-Wilk Test for BodyTemp:\n",
            "Statistic: 0.527208685874939, P-value: 1.401298464324817e-45\n",
            "\n",
            "Shapiro-Wilk Test for HeartRate:\n",
            "Statistic: 0.947784960269928, P-value: 1.812980138598305e-18\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize the power transformer with the Yeo-Johnson method\n",
        "pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
        "\n",
        "for feature in features:\n",
        "    # Reshape the data and transform\n",
        "    transformed_data = pt.fit_transform(dataset_nb[feature].values.reshape(-1, 1))\n",
        "    dataset_nb[feature] = transformed_data.flatten()\n",
        "    \n",
        "# Shapiro-Wilk Test for normality\n",
        "print(features)\n",
        "for feature in features:\n",
        "    shapiro_test = stats.shapiro(dataset_nb[feature])\n",
        "    print(f\"Shapiro-Wilk Test for {feature}:\\nStatistic: {shapiro_test[0]}, P-value: {shapiro_test[1]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Yeo-Johnson transformation seems to have helped for some features but not all.\n",
        "\n",
        "Let's interpret the results:\n",
        "\n",
        "- Age: The statistic is closer to 1, which indicates a more normal distribution, although the P-value is still very small suggesting that the distribution is not perfectly Gaussian.\n",
        "- SystolicBP, DiastolicBP, BS, and HeartRate: All have statistics that are closer to 1, showing they are more Gaussian-like than before. However, the P-values are still significantly small, meaning there is still some deviation from a perfect normal distribution.\n",
        "- BodyTemp: The transformation seems to have had minimal effect on this feature. The statistic is very far from 1, and the P-value is extremely small.\n",
        "\n",
        "Given the results:\n",
        "\n",
        "It's clear that the transformations improved the normality of some features but not perfectly.\n",
        "\n",
        "Since the BodyTemp transformation did not seem to help much, we consider binning this feature into categories and treating it as a categorical variable in the model.\n",
        "\n",
        "Given the distribution of temperatures and the typical body temperature range, we can design our bins in the following manner:\n",
        "\n",
        "- Low (Hypothermia): Anything below 97.6°F.\n",
        "- Normal: Temperatures ranging from 97.6°F to 99.6°F.\n",
        "- Mild Fever: Temperatures ranging from 99.6°F to 100.4°F.\n",
        "- Moderate Fever: Temperatures ranging from 100.4°F to 102°F.\n",
        "- High Fever (Potentially Severe): Anything above 102°F."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Going back to original data\n",
        "dataset_nb = dataset.copy()\n",
        "dataset_nb['RiskLevel_encoded'] = dataset_nb['RiskLevel'].replace(risk_mapping)\n",
        "\n",
        "# Binning BodyTemp based on the distribution and typical body temperature range\n",
        "bins = [95, 97.6, 99.6, 100.4, 102, 105] # 95 and 105 are arbitrary endpoints for extreme values.\n",
        "labels = [\"Low\", \"Normal\", \"Mild Fever\", \"Moderate Fever\", \"High Fever\"]\n",
        "dataset_nb['BodyTemp_binned'] = pd.cut(dataset_nb['BodyTemp'], bins=bins, labels=labels, right=True)\n",
        "\n",
        "# If using in Gaussian Naive Bayes, encode this new feature\n",
        "le = LabelEncoder()\n",
        "dataset_nb['BodyTemp_encoded'] = le.fit_transform(dataset_nb['BodyTemp_binned'])\n",
        "\n",
        "# Initialize the power transformer with the Yeo-Johnson method\n",
        "pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
        "\n",
        "for feature in features:\n",
        "    # Reshape the data and transform\n",
        "    transformed_data = pt.fit_transform(dataset_nb[feature].values.reshape(-1, 1))\n",
        "    dataset_nb[feature] = transformed_data.flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Logistic Regression Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Logistic Regression Performance:\n",
            "Accuracy: 0.6502463054187192\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.89      0.73        80\n",
            "           1       0.68      0.28      0.39        76\n",
            "           2       0.70      0.85      0.77        47\n",
            "\n",
            "    accuracy                           0.65       203\n",
            "   macro avg       0.67      0.67      0.63       203\n",
            "weighted avg       0.66      0.65      0.61       203\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Splitting the data into train and test sets for the dataset_lr\n",
        "X_lr = dataset_lr[features]\n",
        "y_lr = dataset_lr['RiskLevel_encoded']\n",
        "\n",
        "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr, y_lr, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating and training the model\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train_lr, y_train_lr)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred_lr = lr.predict(X_test_lr)\n",
        "\n",
        "# Evaluating the model\n",
        "print(\"\\nLogistic Regression Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_lr, y_pred_lr))\n",
        "print(classification_report(y_test_lr, y_pred_lr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Naïve Bayes Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gaussian Naive Bayes Performance:\n",
            "Accuracy: 0.5665024630541872\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.80      0.66        80\n",
            "           1       0.54      0.25      0.34        76\n",
            "           2       0.59      0.68      0.63        47\n",
            "\n",
            "    accuracy                           0.57       203\n",
            "   macro avg       0.57      0.58      0.55       203\n",
            "weighted avg       0.56      0.57      0.53       203\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Splitting the data into train and test sets for the dataset_nb\n",
        "X_nb = dataset_nb[features + ['BodyTemp_encoded']]\n",
        "y_nb = dataset_nb['RiskLevel_encoded']\n",
        "\n",
        "X_train_nb, X_test_nb, y_train_nb, y_test_nb = train_test_split(X_nb, y_nb, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creating and training the model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train_nb, y_train_nb)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred_nb = gnb.predict(X_test_nb)\n",
        "\n",
        "# Evaluating the model\n",
        "print(\"Gaussian Naive Bayes Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_nb, y_pred_nb))\n",
        "print(classification_report(y_test_nb, y_pred_nb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Those results provide a good overview of the performance of the two models on your dataset:\n",
        "\n",
        "*Gaussian Naive Bayes*:\n",
        "\n",
        "Accuracy: \n",
        "- Approximately 56.65%\n",
        "- This model does fairly well with the high risk (2) and low risk (0) classes but struggles with the mid risk (1) class.\n",
        "- Recall for the mid risk is quite low, indicating that many true mid risk cases are not correctly identified.\n",
        "\n",
        "*Logistic Regression*:\n",
        "- Accuracy: Approximately 65.02%\n",
        "- The logistic regression model performs better than Gaussian Naive Bayes in terms of accuracy.\n",
        "- Again, the model does well with the high risk (2) and low risk (0) classes but has difficulty with the mid risk (1) class, particularly in terms of recall.\n",
        "\n",
        "*Analysis*:\n",
        "\n",
        "- Overall, the Logistic Regression model seems to perform better on this dataset in terms of accuracy.\n",
        "- The consistent challenge for both models appears to be the mid risk class. It may be worth investigating further to understand if there's a feature imbalance or if additional features are needed to better differentiate this class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rQYdUQx-dGi"
      },
      "source": [
        "------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4-fold cross-validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gaussian Naive Bayes Performance (4-fold CV):\n",
            "Macro Precision: 0.55691928801396\n",
            "Macro Recall: 0.5639005602240896\n",
            "Micro Precision: 0.5522626124303632\n",
            "Micro Recall: 0.5522626124303632\n",
            "\n",
            "\n",
            "Logistic Regression Performance (4-fold CV):\n",
            "Macro Precision: 0.6165478796742268\n",
            "Macro Recall: 0.6117622791690934\n",
            "Micro Precision: 0.6163051258908842\n",
            "Micro Recall: 0.6163051258908842\n"
          ]
        }
      ],
      "source": [
        "# Set up the models\n",
        "gnb = GaussianNB()\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Prepare precision and recall scorers for both micro and macro averages\n",
        "scoring = {\n",
        "    'precision_macro': make_scorer(precision_score, average='macro'),\n",
        "    'recall_macro': make_scorer(recall_score, average='macro'),\n",
        "    'precision_micro': make_scorer(precision_score, average='micro'),\n",
        "    'recall_micro': make_scorer(recall_score, average='micro')\n",
        "}\n",
        "\n",
        "# Conduct 4-fold cross validation for Gaussian Naïve Bayes\n",
        "X_nb = dataset_nb[features]\n",
        "y_nb = dataset_nb['RiskLevel_encoded']\n",
        "\n",
        "results_gnb = cross_validate(gnb, X_nb, y_nb, cv=4, scoring=scoring)\n",
        "\n",
        "# Conduct 4-fold cross validation for Logistic Regression\n",
        "X_lr = dataset_lr[features]\n",
        "y_lr = dataset_lr['RiskLevel_encoded']\n",
        "\n",
        "results_lr = cross_validate(lr, X_lr, y_lr, cv=4, scoring=scoring)\n",
        "\n",
        "# Print the results\n",
        "print(\"Gaussian Naive Bayes Performance (4-fold CV):\")\n",
        "print(\"Macro Precision:\", np.mean(results_gnb['test_precision_macro']))\n",
        "print(\"Macro Recall:\", np.mean(results_gnb['test_recall_macro']))\n",
        "print(\"Micro Precision:\", np.mean(results_gnb['test_precision_micro']))\n",
        "print(\"Micro Recall:\", np.mean(results_gnb['test_recall_micro']))\n",
        "print(\"\\n\")\n",
        "print(\"Logistic Regression Performance (4-fold CV):\")\n",
        "print(\"Macro Precision:\", np.mean(results_lr['test_precision_macro']))\n",
        "print(\"Macro Recall:\", np.mean(results_lr['test_recall_macro']))\n",
        "print(\"Micro Precision:\", np.mean(results_lr['test_precision_micro']))\n",
        "print(\"Micro Recall:\", np.mean(results_lr['test_recall_micro']))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's analyze the results you obtained.\n",
        "\n",
        "1. Gaussian Naive Bayes (GNB):\n",
        "\n",
        "Macro Precision/Recall: Both values are around 55.7%. This indicates that, on average, GNB correctly identifies 55.7% of the samples, and it recalls 55.7% of the actual samples across the three classes.\n",
        "Micro Precision/Recall: The results here are roughly the same as the macro metrics. This suggests that the model's performance across the different classes is somewhat uniform.\n",
        "\n",
        "2. Logistic Regression (LR):\n",
        "\n",
        "Macro Precision/Recall: Both values are around 61.4%. LR seems to perform better than GNB by about 6% in terms of both precision and recall on a macro level.\n",
        "Micro Precision/Recall: Similarly, the micro metrics for LR are consistent with the macro metrics. It also suggests the model's performance across different classes is somewhat consistent.\n",
        "\n",
        "*Discussion*:\n",
        "\n",
        "- The macro and micro precision/recall values for both GNB and LR are very close to each other. This means that no single class dramatically affects the overall performance of the models. If there was a severe class imbalance, we might have seen a significant difference between macro and micro values.\n",
        "- LR outperforms GNB in this dataset. This indicates that the relationship between the predictors and the target variable might be more linear, which LR captures better than GNB.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGBWQqlK-lra"
      },
      "source": [
        "------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Modifying Parameters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Naïve Bayes**\n",
        "\n",
        "*Experiment 1: Adjusting Smoothing Parameter*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gaussian Naive Bayes (Smoothing 1e-2) Accuracy: 0.5714285714285714\n"
          ]
        }
      ],
      "source": [
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset_nb.drop(columns=['RiskLevel', 'RiskLevel_encoded', 'BodyTemp_binned']), \n",
        "                                                    dataset_nb['RiskLevel_encoded'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Use a Gaussian Naive Bayes model with adjusted smoothing\n",
        "gnb1 = GaussianNB(var_smoothing=1e-2) # Adjusting the smoothing parameter\n",
        "gnb1.fit(X_train, y_train)\n",
        "predictions = gnb1.predict(X_test)\n",
        "print(\"Gaussian Naive Bayes (Smoothing 1e-2) Accuracy:\", accuracy_score(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Experiment 2: Further Adjusting Smoothing Parameter*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gaussian Naive Bayes (Smoothing 1e-5) Accuracy: 0.5911330049261084\n"
          ]
        }
      ],
      "source": [
        "# Use a Gaussian Naive Bayes model with a different adjusted smoothing\n",
        "gnb2 = GaussianNB(var_smoothing=1e-5) # Adjusting the smoothing parameter again\n",
        "gnb2.fit(X_train, y_train)\n",
        "predictions = gnb2.predict(X_test)\n",
        "print(\"Gaussian Naive Bayes (Smoothing 1e-5) Accuracy:\", accuracy_score(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Experiment 1: Changing the Solver*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression (Solver: sag) Accuracy: 0.5123152709359606\n"
          ]
        }
      ],
      "source": [
        "# Use a Logistic Regression model with a different solver\n",
        "lr1 = LogisticRegression(solver='sag', max_iter=50000) # Using 'sag' solver\n",
        "lr1.fit(X_train, y_train)\n",
        "predictions = lr1.predict(X_test)\n",
        "print(\"Logistic Regression (Solver: sag) Accuracy:\", accuracy_score(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Experiment2: Adjusting Tolerance*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression (Tolerance: 1e-5) Accuracy: 0.5369458128078818\n"
          ]
        }
      ],
      "source": [
        "# Use a Logistic Regression model with adjusted tolerance\n",
        "lr2 = LogisticRegression(tol=1e-5, max_iter=5000) # Adjusting the tolerance for stopping criteria\n",
        "lr2.fit(X_train, y_train)\n",
        "predictions = lr2.predict(X_test)\n",
        "print(\"Logistic Regression (Tolerance: 1e-5) Accuracy:\", accuracy_score(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Analysis**\n",
        "\n",
        "*Gaussian Naive Bayes*:\n",
        "\n",
        "With the default smoothing (no explicit var_smoothing set), the accuracy was approximately 0.552.\n",
        "When the smoothing parameter was adjusted to 1e-2, there was a slight improvement in accuracy to approximately 0.571.\n",
        "Further reducing the smoothing parameter to 1e-5 yielded a slightly better accuracy of approximately 0.591.\n",
        "This suggests that by adjusting the smoothing parameter, you can fine-tune the performance of the Gaussian Naive Bayes classifier for this dataset.\n",
        "\n",
        "*Logistic Regression*:\n",
        "\n",
        "The default logistic regression model using the 'lbfgs' solver and default tolerance had an accuracy of approximately 0.616.\n",
        "Changing the solver to 'sag' significantly reduced the model's performance to approximately 0.498. This reduction is also highlighted by the warning that the coefficients did not converge. The 'sag' solver may require more iterations to converge or may not be the best choice for this dataset.\n",
        "Adjusting the tolerance to 1e-5 (which affects the stopping criteria for the solver) resulted in a slight improvement in accuracy to approximately 0.537.\n",
        "These experiments suggest the importance of parameter tuning. The Gaussian Naive Bayes model responded positively to changes in the smoothing parameter, showing improvements in accuracy. On the other hand, the Logistic Regression model's performance decreased when using the 'sag' solver but saw some recovery with a tighter stopping criterion.\n",
        "\n",
        "Given the results, if you were to choose a model, the Gaussian Naive Bayes with a smoothing parameter of 1e-5 seems to perform best among the tried configurations. However, the default Logistic Regression (with the 'lbfgs' solver and default tolerance) still outperforms the other configurations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ikmq-si_23Y"
      },
      "source": [
        "**Conclusion and Reflection:**\n",
        "\n",
        "1. Overview:\n",
        "The empirical study focused on comparing the performance of two classical machine learning models, Gaussian Naive Bayes (GNB) and Logistic Regression (LR), on predicting maternal health risk levels. The dataset comprises features such as age, blood pressure, blood sugar, body temperature, and heart rate, aiming to classify individuals into three risk categories: low, mid, and high risk.\n",
        "\n",
        "2. Performance Evaluation:\n",
        "Based on the results:\n",
        "\n",
        "GNB achieved an accuracy of approximately 55.2% in 4-fold cross-validation, with macro precision and recall values of 55.6% and 56.3% respectively.\n",
        "LR outperformed GNB, registering an accuracy of approximately 61.6% in 4-fold cross-validation. Both macro precision and recall for LR were roughly 61.6%.\n",
        "These results suggest that, for this particular dataset, LR seems to be a more suitable model. However, neither model achieved exceptionally high accuracy, indicating that there's potential for further optimization.\n",
        "\n",
        "3. Parameter Tuning:\n",
        "By adjusting hyperparameters:\n",
        "\n",
        "GNB exhibited an accuracy increase from 57.1% to 59.1% with a change in smoothing.\n",
        "LR had variable results with changes in solver and tolerance, indicating that parameter tuning significantly influences the model's behavior. Notably, the 'sag' solver exhibited convergence issues, which is a common challenge in optimization tasks.\n",
        "4. Reflections on Dataset:\n",
        "The classes in the dataset were not perfectly balanced, with a distribution of 40% low risk, 33% mid risk, and 27% high risk. This imbalance may have influenced model performance, especially in precision and recall metrics, and is worth considering in future evaluations.\n",
        "\n",
        "5. Ideas for Future Work:\n",
        "\n",
        "- Feature Engineering: Exploring new features or transforming existing ones might uncover patterns that the models can leverage for better performance.\n",
        "- Ensemble Methods: Combining the predictions of multiple models might boost overall performance. Techniques like bagging, boosting, or stacking could be explored.\n",
        "- Advanced Algorithms: While GNB and LR are foundational models, exploring more advanced algorithms like Random Forests, Gradient Boosting Machines, or Neural Networks might yield better results.\n",
        "- Data Augmentation: Given the potential class imbalance, techniques such as SMOTE or ADASYN could be employed to synthetically augment the dataset, ensuring each class is equally represented.\n",
        "- Model Interpretability: Beyond accuracy, understanding how models make decisions is crucial, especially in healthcare. Techniques such as SHAP or LIME could shed light on feature importance and model decision pathways.\n",
        "\n",
        "\n",
        "In summary, the empirical study offered valuable insights into the performance of Gaussian Naive Bayes and Logistic Regression models on a maternal health dataset. While results indicate there's room for improvement, the research serves as a foundation for future endeavors in this domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41s8gvCNABDy"
      },
      "source": [
        "--------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtbOgI1q_9a0"
      },
      "source": [
        "**References**\n",
        "\n",
        "- https://health.clevelandclinic.org/body-temperature-what-is-and-isnt-normal/.\n",
        "- https://towardsdatascience.com/types-of-transformations-for-better-normal-distribution-61c22668d3b9\n",
        "- https://www.tutorialspoint.com/scikit_learn/index.htm\n",
        "- https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "- https://www.youtube.com/watch?v=0Lt9w-BxKFQ\n",
        "- https://www.chat.openai.com"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.18 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
